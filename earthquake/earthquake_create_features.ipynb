{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "earthquake_create_features.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berndheidemann/notebooks_fastai/blob/master/earthquake/earthquake_create_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PatDy4--xa6w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Features from https://www.kaggle.com/artgor/earthquakes-fe-more-features-and-samples"
      ]
    },
    {
      "metadata": {
        "id": "C2tk2XcazNzs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# get data"
      ]
    },
    {
      "metadata": {
        "id": "IsEDkdemxeHS",
        "colab_type": "code",
        "outputId": "e1875e28-915a-4b2a-da40-03177652b2d6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0e2c112f-9087-46e8-84ac-41fd06b9f86a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0e2c112f-9087-46e8-84ac-41fd06b9f86a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"berndhe\",\"key\":\"fdec68eaf76e49902126e78294049090\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "CuPh-xw6xmI3",
        "colab_type": "code",
        "outputId": "209d9292-ef0e-4927-f7c3-faadfe1bfc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!kaggle competitions download -c LANL-Earthquake-Prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/33.3k [00:00<?, ?B/s]\n",
            "100% 33.3k/33.3k [00:00<00:00, 13.6MB/s]\n",
            "Downloading test.zip to /content\n",
            " 95% 230M/242M [00:01<00:00, 126MB/s]\n",
            "100% 242M/242M [00:01<00:00, 142MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "100% 2.02G/2.03G [00:41<00:00, 36.3MB/s]\n",
            "100% 2.03G/2.03G [00:41<00:00, 52.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aHItF3hgxpdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir test\n",
        "!unzip test.zip -d test\n",
        "!unzip train.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlhOtb6ezCZd",
        "colab_type": "code",
        "outputId": "5476d8eb-110e-4cf2-d21c-a558f27518ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip train.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fzLCAJtTzSwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# create features"
      ]
    },
    {
      "metadata": {
        "id": "sq2gx5MZxa65",
        "colab_type": "code",
        "outputId": "82bed32e-137d-48a9-e660-c23bd3812dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install tsfresh\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import NuSVR, SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "pd.options.display.precision = 15\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import time\n",
        "import datetime\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import gc\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from scipy.signal import hilbert\n",
        "from scipy.signal import hann\n",
        "from scipy.signal import convolve\n",
        "from scipy import stats\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from tsfresh.feature_extraction import feature_calculators\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import scipy as sp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: enum34 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.6)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.23.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2018.9)\n",
            "Requirement already satisfied: tsfresh in /usr/local/lib/python3.6/dist-packages (0.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (1.12.0)\n",
            "Requirement already satisfied: dask>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (1.2.1)\n",
            "Requirement already satisfied: distributed>=1.18.3 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (1.25.3)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (1.16.3)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (0.16.0)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (4.28.1)\n",
            "Requirement already satisfied: pandas<=0.23.4,>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (0.23.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (0.20.3)\n",
            "Requirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tsfresh) (0.9.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (0.1.4)\n",
            "Requirement already satisfied: tblib in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (1.3.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (2.1.0)\n",
            "Requirement already satisfied: tornado>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (4.5.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (7.0)\n",
            "Requirement already satisfied: toolz>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (0.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (3.13)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (0.5.6)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=1.18.3->tsfresh) (5.4.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->tsfresh) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->tsfresh) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->tsfresh) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->tsfresh) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas<=0.23.4,>=0.20.3->tsfresh) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas<=0.23.4,>=0.20.3->tsfresh) (2018.9)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=1.18.3->tsfresh) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ErsBO6eCWKmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_trend_feature(arr, abs_values=False):\n",
        "    idx = np.array(range(len(arr)))\n",
        "    if abs_values:\n",
        "        arr = np.abs(arr)\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(idx.reshape(-1, 1), arr)\n",
        "    return lr.coef_[0]\n",
        "\n",
        "def classic_sta_lta(x, length_sta, length_lta):\n",
        "    \n",
        "    sta = np.cumsum(x ** 2)\n",
        "\n",
        "    # Convert to float\n",
        "    sta = np.require(sta, dtype=np.float)\n",
        "\n",
        "    # Copy for LTA\n",
        "    lta = sta.copy()\n",
        "\n",
        "    # Compute the STA and the LTA\n",
        "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
        "    sta /= length_sta\n",
        "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
        "    lta /= length_lta\n",
        "\n",
        "    # Pad zeros\n",
        "    sta[:length_lta - 1] = 0\n",
        "\n",
        "    # Avoid division by zero by setting zero values to tiny float\n",
        "    dtiny = np.finfo(0.0).tiny\n",
        "    idx = lta < dtiny\n",
        "    lta[idx] = dtiny\n",
        "\n",
        "    return sta / lta\n",
        "  \n",
        "\n",
        "def calc_change_rate(x):\n",
        "    change = (np.diff(x) / x[:-1]).values\n",
        "    change = change[np.nonzero(change)[0]]\n",
        "    change = change[~np.isnan(change)]\n",
        "    change = change[change != -np.inf]\n",
        "    change = change[change != np.inf]\n",
        "    return np.mean(change)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X1t-uyqzV548",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeatureGenerator(object):\n",
        "    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.dtype = dtype\n",
        "        self.filename = None\n",
        "        self.n_jobs = n_jobs\n",
        "        self.test_files = []\n",
        "        if self.dtype == 'train':\n",
        "            self.filename = 'train.csv'\n",
        "            self.total_data = int(629145481 / self.chunk_size)\n",
        "        else:\n",
        "            submission = pd.read_csv('sample_submission.csv')\n",
        "            for seg_id in submission.seg_id.values:\n",
        "                self.test_files.append((seg_id, 'test/' + seg_id + '.csv'))\n",
        "            self.total_data = int(len(submission))\n",
        "\n",
        "    def read_chunks(self):\n",
        "        if self.dtype == 'train':\n",
        "            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n",
        "                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n",
        "            for counter, df in enumerate(iter_df):\n",
        "                x = df.acoustic_data.values\n",
        "                y = df.time_to_failure.values[-1]\n",
        "                seg_id = 'train_' + str(counter)\n",
        "                del df\n",
        "                yield seg_id, x, y\n",
        "        else:\n",
        "            for seg_id, f in self.test_files:\n",
        "                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n",
        "                x = df.acoustic_data.values[-self.chunk_size:]\n",
        "                del df\n",
        "                yield seg_id, x, -999\n",
        "\n",
        "    def features(self, x, y, seg_id):\n",
        "        feature_dict = dict()\n",
        "        feature_dict['target'] = y\n",
        "        feature_dict['seg_id'] = seg_id\n",
        "\n",
        "        # create features here\n",
        "        # numpy\n",
        "        feature_dict['mean'] = np.mean(x)\n",
        "        feature_dict['max'] = np.max(x)\n",
        "        feature_dict['min'] = np.min(x)\n",
        "        feature_dict['std'] = np.std(x)\n",
        "        feature_dict['var'] = np.var(x)\n",
        "        feature_dict['ptp'] = np.ptp(x)\n",
        "        feature_dict['percentile_10'] = np.percentile(x, 10)\n",
        "        feature_dict['percentile_20'] = np.percentile(x, 20)\n",
        "        feature_dict['percentile_30'] = np.percentile(x, 30)\n",
        "        feature_dict['percentile_40'] = np.percentile(x, 40)\n",
        "        feature_dict['percentile_50'] = np.percentile(x, 50)\n",
        "        feature_dict['percentile_60'] = np.percentile(x, 60)\n",
        "        feature_dict['percentile_70'] = np.percentile(x, 70)\n",
        "        feature_dict['percentile_80'] = np.percentile(x, 80)\n",
        "        feature_dict['percentile_90'] = np.percentile(x, 90)\n",
        "\n",
        "        # scipy\n",
        "        feature_dict['skew'] = sp.stats.skew(x)\n",
        "        feature_dict['kurtosis'] = sp.stats.kurtosis(x)\n",
        "        feature_dict['kstat_1'] = sp.stats.kstat(x, 1)\n",
        "        feature_dict['kstat_2'] = sp.stats.kstat(x, 2)\n",
        "        feature_dict['kstat_3'] = sp.stats.kstat(x, 3)\n",
        "        feature_dict['kstat_4'] = sp.stats.kstat(x, 4)\n",
        "        feature_dict['moment_1'] = sp.stats.moment(x, 1)\n",
        "        feature_dict['moment_2'] = sp.stats.moment(x, 2)\n",
        "        feature_dict['moment_3'] = sp.stats.moment(x, 3)\n",
        "        feature_dict['moment_4'] = sp.stats.moment(x, 4)\n",
        "        \n",
        "        feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n",
        "        feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n",
        "        feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n",
        "        feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n",
        "        feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n",
        "        feature_dict['mean_change'] = feature_calculators.mean_change(x)\n",
        "        feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n",
        "        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n",
        "        feature_dict['range_m4000_m3000'] = feature_calculators.range_count(x, -4000, -3000)\n",
        "        feature_dict['range_m3000_m2000'] = feature_calculators.range_count(x, -3000, -2000)\n",
        "        feature_dict['range_m2000_m1000'] = feature_calculators.range_count(x, -2000, -1000)\n",
        "        feature_dict['range_m1000_0'] = feature_calculators.range_count(x, -1000, 0)\n",
        "        feature_dict['range_0_p1000'] = feature_calculators.range_count(x, 0, 1000)\n",
        "        feature_dict['range_p1000_p2000'] = feature_calculators.range_count(x, 1000, 2000)\n",
        "        feature_dict['range_p2000_p3000'] = feature_calculators.range_count(x, 2000, 3000)\n",
        "        feature_dict['range_p3000_p4000'] = feature_calculators.range_count(x, 3000, 4000)\n",
        "        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n",
        "\n",
        "        feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n",
        "        feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n",
        "        feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n",
        "        feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n",
        "        feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n",
        "        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n",
        "        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)\n",
        "        feature_dict['time_rev_asym_stat_1000'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1000)\n",
        "        feature_dict['autocorrelation_5'] = feature_calculators.autocorrelation(x, 5)\n",
        "        feature_dict['autocorrelation_10'] = feature_calculators.autocorrelation(x, 10)\n",
        "        feature_dict['autocorrelation_50'] = feature_calculators.autocorrelation(x, 50)\n",
        "        feature_dict['autocorrelation_100'] = feature_calculators.autocorrelation(x, 100)\n",
        "        feature_dict['autocorrelation_1000'] = feature_calculators.autocorrelation(x, 1000)\n",
        "        feature_dict['c3_5'] = feature_calculators.c3(x, 5)\n",
        "        feature_dict['c3_10'] = feature_calculators.c3(x, 10)\n",
        "        feature_dict['c3_100'] = feature_calculators.c3(x, 100)\n",
        "        feature_dict['fft_1_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'real'}]))[0][1]\n",
        "        feature_dict['fft_1_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'imag'}]))[0][1]\n",
        "        feature_dict['fft_1_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'angle'}]))[0][1]\n",
        "        feature_dict['fft_2_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'real'}]))[0][1]\n",
        "        feature_dict['fft_2_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'imag'}]))[0][1]\n",
        "        feature_dict['fft_2_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'angle'}]))[0][1]\n",
        "        feature_dict['fft_3_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'real'}]))[0][1]\n",
        "        feature_dict['fft_3_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'imag'}]))[0][1]\n",
        "        feature_dict['fft_3_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'angle'}]))[0][1]\n",
        "        feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n",
        "        feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n",
        "        feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n",
        "        feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n",
        "        feature_dict['binned_entropy_5'] = feature_calculators.binned_entropy(x, 5)\n",
        "        feature_dict['binned_entropy_10'] = feature_calculators.binned_entropy(x, 10)\n",
        "        feature_dict['binned_entropy_20'] = feature_calculators.binned_entropy(x, 20)\n",
        "        feature_dict['binned_entropy_50'] = feature_calculators.binned_entropy(x, 50)\n",
        "        feature_dict['binned_entropy_80'] = feature_calculators.binned_entropy(x, 80)\n",
        "        feature_dict['binned_entropy_100'] = feature_calculators.binned_entropy(x, 100)\n",
        "\n",
        "        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n",
        "        feature_dict['num_peaks_10'] = feature_calculators.number_peaks(x, 10)\n",
        "        feature_dict['num_peaks_50'] = feature_calculators.number_peaks(x, 50)\n",
        "        feature_dict['num_peaks_100'] = feature_calculators.number_peaks(x, 100)\n",
        "        feature_dict['num_peaks_500'] = feature_calculators.number_peaks(x, 500)\n",
        "\n",
        "        feature_dict['spkt_welch_density_1'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 1}]))[0][1]\n",
        "        feature_dict['spkt_welch_density_10'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 10}]))[0][1]\n",
        "        feature_dict['spkt_welch_density_50'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 50}]))[0][1]\n",
        "        feature_dict['spkt_welch_density_100'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 100}]))[0][1]\n",
        "\n",
        "        feature_dict['time_rev_asym_stat_1'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1)\n",
        "        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n",
        "        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)        \n",
        "\n",
        "        \n",
        "        x = pd.Series(x)\n",
        "        feature_dict['mean_change_abs'] = np.mean(np.diff(x))\n",
        "        feature_dict['mean_change_rate'] = calc_change_rate(x)\n",
        "        feature_dict['abs_max'] = np.abs(x).max()\n",
        "        feature_dict['abs_min'] = np.abs(x).min()\n",
        "\n",
        "        feature_dict['std_first_50000'] = x[:50000].std()\n",
        "        feature_dict['std_last_50000'] = x[-50000:].std()\n",
        "        feature_dict['std_first_10000'] = x[:10000].std()\n",
        "        feature_dict['std_last_10000'] = x[-10000:].std()\n",
        "\n",
        "        feature_dict['avg_first_50000'] = x[:50000].mean()\n",
        "        feature_dict['avg_last_50000'] = x[-50000:].mean()\n",
        "        feature_dict['avg_first_10000'] = x[:10000].mean()\n",
        "        feature_dict['avg_last_10000'] = x[-10000:].mean()\n",
        "\n",
        "        feature_dict['min_first_50000'] = x[:50000].min()\n",
        "        feature_dict['min_last_50000'] = x[-50000:].min()\n",
        "        feature_dict['min_first_10000'] = x[:10000].min()\n",
        "        feature_dict['min_last_10000'] = x[-10000:].min()\n",
        "\n",
        "        feature_dict['max_first_50000'] = x[:50000].max()\n",
        "        feature_dict['max_last_50000'] = x[-50000:].max()\n",
        "        feature_dict['max_first_10000'] = x[:10000].max()\n",
        "        feature_dict['max_last_10000'] = x[-10000:].max()\n",
        "\n",
        "        feature_dict['max_to_min'] = x.max() / np.abs(x.min())\n",
        "        feature_dict['max_to_min_diff'] = x.max() - np.abs(x.min())\n",
        "        feature_dict['count_big'] = len(x[np.abs(x) > 500])\n",
        "        feature_dict['sum'] = x.sum()\n",
        "\n",
        "        feature_dict['mean_change_rate_first_50000'] = calc_change_rate(x[:50000])\n",
        "        feature_dict['mean_change_rate_last_50000'] = calc_change_rate(x[-50000:])\n",
        "        feature_dict['mean_change_rate_first_10000'] = calc_change_rate(x[:10000])\n",
        "        feature_dict['mean_change_rate_last_10000'] = calc_change_rate(x[-10000:])\n",
        "\n",
        "        feature_dict['q95'] = np.quantile(x, 0.95)\n",
        "        feature_dict['q99'] = np.quantile(x, 0.99)\n",
        "        feature_dict['q05'] = np.quantile(x, 0.05)\n",
        "        feature_dict['q01'] = np.quantile(x, 0.01)\n",
        "\n",
        "        feature_dict['abs_q95'] = np.quantile(np.abs(x), 0.95)\n",
        "        feature_dict['abs_q99'] = np.quantile(np.abs(x), 0.99)\n",
        "        feature_dict['abs_q05'] = np.quantile(np.abs(x), 0.05)\n",
        "        feature_dict['abs_q01'] = np.quantile(np.abs(x), 0.01)\n",
        "\n",
        "        feature_dict['trend'] = add_trend_feature(x)\n",
        "        feature_dict['abs_trend'] = add_trend_feature(x, abs_values=True)\n",
        "        feature_dict['abs_mean'] = np.abs(x).mean()\n",
        "        feature_dict['abs_std'] = np.abs(x).std()\n",
        "\n",
        "        feature_dict['mad'] = x.mad()\n",
        "        feature_dict['kurt'] = x.kurtosis()\n",
        "        feature_dict['skew'] = x.skew()\n",
        "        feature_dict['med'] = x.median()\n",
        "\n",
        "        feature_dict['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
        "        feature_dict['Hann_window_mean'] = (convolve(x, hann(150), mode='same') / sum(hann(150))).mean()\n",
        "        feature_dict['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
        "        feature_dict['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
        "        feature_dict['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
        "        feature_dict['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
        "        feature_dict['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n",
        "        feature_dict['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n",
        "        feature_dict['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n",
        "        feature_dict['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n",
        "        feature_dict['classic_sta_lta9_mean'] = classic_sta_lta(x, 30, 2000).mean()\n",
        "        feature_dict['classic_sta_lta10_mean'] = classic_sta_lta(x, 20, 500).mean()\n",
        "        feature_dict['classic_sta_lta11_mean'] = classic_sta_lta(x, 50, 2000).mean()\n",
        "        feature_dict['classic_sta_lta12_mean'] = classic_sta_lta(x, 70, 1500).mean()\n",
        "        feature_dict['classic_sta_lta13_mean'] = classic_sta_lta(x, 100, 800).mean()\n",
        "\n",
        "        feature_dict['Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n",
        "        ewma = pd.Series.ewm\n",
        "        feature_dict['exp_Moving_average_300_mean'] = (ewma(x, span=300).mean()).mean(skipna=True)\n",
        "        feature_dict['exp_Moving_average_3000_mean'] = ewma(x, span=3000).mean().mean(skipna=True)\n",
        "        feature_dict['exp_Moving_average_30000_mean'] = ewma(x, span=30000).mean().mean(skipna=True)\n",
        "        no_of_std = 3\n",
        "        feature_dict['MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n",
        "        feature_dict['MA_400MA_std_mean'] = x.rolling(window=400).std().mean()\n",
        "        feature_dict['MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n",
        "\n",
        "        feature_dict['iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
        "        feature_dict['q999'] = np.quantile(x,0.999)\n",
        "        feature_dict['q001'] = np.quantile(x,0.001)\n",
        "        feature_dict['ave10'] = stats.trim_mean(x, 0.1)\n",
        "\n",
        "        for windows in [10, 100, 1000]:\n",
        "            x_roll_std = x.rolling(windows).std().dropna().values\n",
        "            x_roll_mean = x.rolling(windows).mean().dropna().values\n",
        "\n",
        "            feature_dict['ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
        "            feature_dict['std_roll_std_' + str(windows)] = x_roll_std.std()\n",
        "            feature_dict['max_roll_std_' + str(windows)] = x_roll_std.max()\n",
        "            feature_dict['min_roll_std_' + str(windows)] = x_roll_std.min()\n",
        "            feature_dict['q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
        "            feature_dict['q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
        "            feature_dict['q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
        "            feature_dict['q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
        "            feature_dict['av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
        "            feature_dict['av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
        "            feature_dict['abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
        "\n",
        "            feature_dict['ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
        "            feature_dict['std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
        "            feature_dict['max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
        "            feature_dict['min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
        "            feature_dict['q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
        "            feature_dict['q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
        "            feature_dict['q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
        "            feature_dict['q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
        "            feature_dict['av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
        "            feature_dict['av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
        "            feature_dict['abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
        "        return feature_dict\n",
        "\n",
        "    def generate(self):\n",
        "        feature_list = []\n",
        "        res = Parallel(n_jobs=self.n_jobs,\n",
        "                       backend='threading')(delayed(self.features)(x, y, s)\n",
        "                                            for s, x, y in tqdm(self.read_chunks(), total=self.total_data))\n",
        "        for r in res:\n",
        "            feature_list.append(r)\n",
        "        return pd.DataFrame(feature_list)\n",
        "\n",
        "\n",
        "training_fg = FeatureGenerator(dtype='train', n_jobs=10, chunk_size=150000)\n",
        "training_data = training_fg.generate()\n",
        "\n",
        "test_fg = FeatureGenerator(dtype='test', n_jobs=10, chunk_size=150000)\n",
        "test_data = test_fg.generate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZvP9vdaBEAJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "means_dict = {}\n",
        "for col in X_tr.columns:\n",
        "    if X_tr[col].isnull().any():\n",
        "        print(col)\n",
        "        mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n",
        "        X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n",
        "        X_tr[col] = X_tr[col].fillna(mean_value)\n",
        "        means_dict[col] = mean_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FkhvaQEexa7E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_tr)\n",
        "X_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGb0_t4sxa7I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr.to_csv(\"new_train_x_unscaled.csv\")\n",
        "y_tr.to_csv(\"new_train_y.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3Y8yLznxa7M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test.to_csv(\"new_test_x.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}